{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2082f1b7",
   "metadata": {},
   "source": [
    "# Runtime Performance Comparison: Node.js vs Deno vs Bun\n",
    "\n",
    "This notebook provides a comprehensive comparison of Next.js application performance across three JavaScript runtimes:\n",
    "- **Node.js**: The traditional JavaScript runtime\n",
    "- **Deno**: A secure runtime for JavaScript and TypeScript  \n",
    "- **Bun**: A fast all-in-one JavaScript runtime\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll create identical Next.js applications for each runtime, containerize them using Docker, and monitor their CPU and memory usage patterns to determine which runtime performs best under different scenarios.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Docker and Docker Compose installed\n",
    "- Python 3.11+ with required packages\n",
    "- Sufficient system resources to run multiple containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41677df",
   "metadata": {},
   "source": [
    "## 1. Setup Project Structure\n",
    "\n",
    "First, let's verify our project structure and import the necessary libraries for performance monitoring and data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import threading\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Import our custom performance monitor\n",
    "sys.path.append('/app')\n",
    "from performance_monitor import PerformanceMonitor\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"ğŸ“ Working directory: {os.getcwd()}\")\n",
    "print(f\"ğŸ Python version: {sys.version}\")\n",
    "print(f\"ğŸ• Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify project structure\n",
    "project_root = \"/Users/jabbo/node-vs-deno-vs-bun\"\n",
    "expected_dirs = [\"node-nextjs\", \"deno-nextjs\", \"bun-nextjs\", \"monitoring\"]\n",
    "\n",
    "print(\"ğŸ” Checking project structure...\")\n",
    "for directory in expected_dirs:\n",
    "    path = os.path.join(project_root, directory)\n",
    "    if os.path.exists(path):\n",
    "        print(f\"âœ… {directory}/ exists\")\n",
    "        # List key files in each directory\n",
    "        files = os.listdir(path)\n",
    "        key_files = [f for f in files if f in ['package.json', 'deno.json', 'Dockerfile', 'next.config.js']]\n",
    "        if key_files:\n",
    "            print(f\"   ğŸ“„ Key files: {', '.join(key_files)}\")\n",
    "    else:\n",
    "        print(f\"âŒ {directory}/ missing\")\n",
    "\n",
    "# Check if docker-compose.yml exists\n",
    "compose_file = os.path.join(project_root, \"docker-compose.yml\")\n",
    "if os.path.exists(compose_file):\n",
    "    print(\"âœ… docker-compose.yml exists\")\n",
    "else:\n",
    "    print(\"âŒ docker-compose.yml missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0308ea",
   "metadata": {},
   "source": [
    "## 2. Create NextJS Applications\n",
    "\n",
    "Our project structure includes three identical Next.js applications, each configured for a different runtime:\n",
    "\n",
    "1. **node-nextjs/**: Traditional Node.js setup with npm\n",
    "2. **deno-nextjs/**: Deno setup with deno.json configuration  \n",
    "3. **bun-nextjs/**: Bun setup with native Bun package manager\n",
    "\n",
    "Each application runs on a different port:\n",
    "- Node.js: `http://localhost:3001`\n",
    "- Deno: `http://localhost:3002` \n",
    "- Bun: `http://localhost:3003`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7366296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display configuration comparison\n",
    "configurations = {\n",
    "    \"Runtime\": [\"Node.js\", \"Deno\", \"Bun\"],\n",
    "    \"Package Manager\": [\"npm\", \"deno\", \"bun\"],\n",
    "    \"Port\": [3001, 3002, 3003],\n",
    "    \"Config File\": [\"package.json\", \"deno.json\", \"package.json\"],\n",
    "    \"Container Name\": [\"node-nextjs-app\", \"deno-nextjs-app\", \"bun-nextjs-app\"]\n",
    "}\n",
    "\n",
    "config_df = pd.DataFrame(configurations)\n",
    "print(\"ğŸ“‹ Runtime Configuration Summary:\")\n",
    "display(config_df)\n",
    "\n",
    "# Check package.json files to verify Next.js versions\n",
    "print(\"\\nğŸ” Checking Next.js versions across runtimes:\")\n",
    "for runtime in [\"node-nextjs\", \"deno-nextjs\", \"bun-nextjs\"]:\n",
    "    package_path = f\"{project_root}/{runtime}/package.json\"\n",
    "    if os.path.exists(package_path):\n",
    "        with open(package_path, 'r') as f:\n",
    "            package_data = json.load(f)\n",
    "            nextjs_version = package_data.get('dependencies', {}).get('next', 'Not found')\n",
    "            print(f\"  {runtime}: Next.js {nextjs_version}\")\n",
    "    else:\n",
    "        print(f\"  {runtime}: package.json not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d77e49",
   "metadata": {},
   "source": [
    "## 3. Generate Dockerfiles for Each Runtime\n",
    "\n",
    "Each runtime has its own optimized Dockerfile:\n",
    "\n",
    "- **Node.js**: Uses `node:18-alpine` base image with npm\n",
    "- **Deno**: Uses `denoland/deno:1.37.0` with native Deno commands\n",
    "- **Bun**: Uses `oven/bun:1.0.7` with native Bun commands\n",
    "\n",
    "All containers are configured for production builds with the `standalone` output mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f8bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Dockerfile comparison\n",
    "print(\"ğŸ³ Dockerfile Configuration Comparison:\\n\")\n",
    "\n",
    "dockerfiles = {\n",
    "    \"node-nextjs\": f\"{project_root}/node-nextjs/Dockerfile\",\n",
    "    \"deno-nextjs\": f\"{project_root}/deno-nextjs/Dockerfile\", \n",
    "    \"bun-nextjs\": f\"{project_root}/bun-nextjs/Dockerfile\"\n",
    "}\n",
    "\n",
    "dockerfile_info = []\n",
    "for runtime, dockerfile_path in dockerfiles.items():\n",
    "    if os.path.exists(dockerfile_path):\n",
    "        with open(dockerfile_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        # Extract key information\n",
    "        lines = content.split('\\n')\n",
    "        base_image = next((line.split()[1] for line in lines if line.startswith('FROM')), 'Not found')\n",
    "        expose_port = next((line.split()[1] for line in lines if line.startswith('EXPOSE')), 'Not found')\n",
    "        \n",
    "        dockerfile_info.append({\n",
    "            'Runtime': runtime,\n",
    "            'Base Image': base_image,\n",
    "            'Exposed Port': expose_port,\n",
    "            'File Size (bytes)': len(content)\n",
    "        })\n",
    "        \n",
    "        print(f\"ğŸ“„ {runtime}/Dockerfile:\")\n",
    "        print(f\"   Base Image: {base_image}\")\n",
    "        print(f\"   Exposed Port: {expose_port}\")\n",
    "        print(f\"   File Size: {len(content)} bytes\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"âŒ {dockerfile_path} not found\")\n",
    "\n",
    "# Create DataFrame for comparison\n",
    "if dockerfile_info:\n",
    "    dockerfile_df = pd.DataFrame(dockerfile_info)\n",
    "    display(dockerfile_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaa0559",
   "metadata": {},
   "source": [
    "## 4. Create Docker Compose Configuration\n",
    "\n",
    "The Docker Compose file orchestrates all three Next.js applications plus our monitoring container:\n",
    "\n",
    "- **Services**: node-nextjs, deno-nextjs, bun-nextjs, monitoring\n",
    "- **Network**: All containers share the `nextjs-comparison` network\n",
    "- **Health Checks**: Each app container has HTTP health checks\n",
    "- **Volumes**: Monitoring container has access to Docker socket and shared data volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Docker and Docker Compose availability\n",
    "def check_docker():\n",
    "    try:\n",
    "        result = subprocess.run(['docker', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… Docker: {result.stdout.strip()}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ Docker not found\")\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ Docker command not found\")\n",
    "        return False\n",
    "\n",
    "def check_docker_compose():\n",
    "    try:\n",
    "        result = subprocess.run(['docker-compose', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… Docker Compose: {result.stdout.strip()}\")\n",
    "            return True\n",
    "        else:\n",
    "            # Try docker compose (newer syntax)\n",
    "            result = subprocess.run(['docker', 'compose', 'version'], capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                print(f\"âœ… Docker Compose: {result.stdout.strip()}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"âŒ Docker Compose not found\")\n",
    "                return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ Docker Compose command not found\")\n",
    "        return False\n",
    "\n",
    "print(\"ğŸ” Checking Docker environment:\")\n",
    "docker_available = check_docker()\n",
    "compose_available = check_docker_compose()\n",
    "\n",
    "if docker_available and compose_available:\n",
    "    print(\"\\nâœ… Docker environment is ready!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Docker environment setup required before proceeding\")\n",
    "\n",
    "# Display docker-compose.yml structure\n",
    "compose_path = f\"{project_root}/docker-compose.yml\"\n",
    "if os.path.exists(compose_path):\n",
    "    print(f\"\\nğŸ“„ Docker Compose file structure:\")\n",
    "    with open(compose_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Show service definitions\n",
    "    services = []\n",
    "    for line in lines:\n",
    "        if line.strip().endswith(':') and not line.startswith(' ') and 'services' not in line:\n",
    "            service_name = line.strip().rstrip(':')\n",
    "            if service_name not in ['version', 'networks']:\n",
    "                services.append(service_name)\n",
    "    \n",
    "    print(f\"   Services defined: {', '.join(services)}\")\n",
    "    print(f\"   Total lines: {len(lines)}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ {compose_path} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4f7ab7",
   "metadata": {},
   "source": [
    "## 5. Build Performance Monitoring Script\n",
    "\n",
    "Our performance monitoring system includes:\n",
    "\n",
    "- **PerformanceMonitor class**: Monitors Docker containers using the Docker API\n",
    "- **Metrics collection**: CPU percentage, memory usage (MB and %), container health\n",
    "- **Data persistence**: Automatic saving to JSON and CSV formats\n",
    "- **Real-time monitoring**: Configurable interval-based collection\n",
    "- **System-wide metrics**: Overall system resource usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize performance monitor\n",
    "monitor = PerformanceMonitor(interval=5)  # Monitor every 5 seconds\n",
    "\n",
    "print(\"ğŸ”§ Performance Monitor Configuration:\")\n",
    "print(f\"   Monitoring interval: {monitor.interval} seconds\")\n",
    "print(f\"   Target containers: {list(monitor.containers.keys())}\")\n",
    "print(f\"   Data storage path: /app/data/\")\n",
    "\n",
    "# Test monitor connectivity\n",
    "print(\"\\nğŸ” Testing Docker connectivity:\")\n",
    "try:\n",
    "    import docker\n",
    "    client = docker.from_env()\n",
    "    containers = client.containers.list()\n",
    "    print(f\"   âœ… Docker client connected\")\n",
    "    print(f\"   ğŸ“¦ Currently running containers: {len(containers)}\")\n",
    "    \n",
    "    for container in containers:\n",
    "        print(f\"     - {container.name}: {container.status}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Docker connection failed: {e}\")\n",
    "\n",
    "# Show monitor methods\n",
    "print(\"\\nğŸ“‹ Available monitoring methods:\")\n",
    "methods = [method for method in dir(monitor) if not method.startswith('_') and callable(getattr(monitor, method))]\n",
    "for method in methods[:10]:  # Show first 10 methods\n",
    "    print(f\"   - {method}()\")\n",
    "    \n",
    "print(f\"   ... and {len(methods)-10} more methods\" if len(methods) > 10 else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aee78d",
   "metadata": {},
   "source": [
    "## 6. Run Docker Containers and Collect Metrics\n",
    "\n",
    "This section will start the Docker containers and begin performance monitoring. We'll:\n",
    "\n",
    "1. **Build and start all containers** using Docker Compose\n",
    "2. **Wait for containers to be healthy** before starting monitoring\n",
    "3. **Collect performance metrics** for a specified duration\n",
    "4. **Save the data** for analysis\n",
    "\n",
    "**âš ï¸ Important**: This step requires the containers to be running. If you're running this notebook inside the monitoring container, the other containers should already be started via Docker Compose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f2355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check container status and health\n",
    "def check_container_status():\n",
    "    \"\"\"Check the status of all target containers\"\"\"\n",
    "    import docker\n",
    "    client = docker.from_env()\n",
    "    \n",
    "    container_status = {}\n",
    "    for container_name in monitor.containers.keys():\n",
    "        try:\n",
    "            container = client.containers.get(container_name)\n",
    "            container_status[container_name] = {\n",
    "                'status': container.status,\n",
    "                'health': getattr(container.attrs['State'], 'Health', {}).get('Status', 'N/A'),\n",
    "                'ports': container.ports\n",
    "            }\n",
    "        except docker.errors.NotFound:\n",
    "            container_status[container_name] = {\n",
    "                'status': 'not found',\n",
    "                'health': 'N/A',\n",
    "                'ports': {}\n",
    "            }\n",
    "        except Exception as e:\n",
    "            container_status[container_name] = {\n",
    "                'status': f'error: {e}',\n",
    "                'health': 'N/A',\n",
    "                'ports': {}\n",
    "            }\n",
    "    \n",
    "    return container_status\n",
    "\n",
    "# Check current container status\n",
    "print(\"ğŸ” Checking container status:\")\n",
    "status = check_container_status()\n",
    "\n",
    "status_df_data = []\n",
    "for name, info in status.items():\n",
    "    status_df_data.append({\n",
    "        'Container': name,\n",
    "        'Status': info['status'],\n",
    "        'Health': info['health'],\n",
    "        'Ports': str(info['ports']) if info['ports'] else 'None'\n",
    "    })\n",
    "\n",
    "status_df = pd.DataFrame(status_df_data)\n",
    "display(status_df)\n",
    "\n",
    "# Check if containers are ready for monitoring\n",
    "ready_containers = [name for name, info in status.items() if info['status'] == 'running']\n",
    "print(f\"\\nğŸ“Š Containers ready for monitoring: {len(ready_containers)}/{len(monitor.containers)}\")\n",
    "\n",
    "if len(ready_containers) < len(monitor.containers):\n",
    "    print(\"âš ï¸  Some containers are not running. You may need to start them with:\")\n",
    "    print(\"   docker-compose up -d\")\n",
    "else:\n",
    "    print(\"âœ… All containers are running and ready for monitoring!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1413e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive monitoring controls\n",
    "monitoring_duration = widgets.IntSlider(\n",
    "    value=60,\n",
    "    min=30,\n",
    "    max=300,\n",
    "    step=10,\n",
    "    description='Duration (s):',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "monitoring_interval = widgets.IntSlider(\n",
    "    value=5,\n",
    "    min=1,\n",
    "    max=30,\n",
    "    step=1,\n",
    "    description='Interval (s):',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "start_button = widgets.Button(\n",
    "    description='Start Monitoring',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    tooltip='Start performance monitoring',\n",
    "    icon='play'\n",
    ")\n",
    "\n",
    "stop_button = widgets.Button(\n",
    "    description='Stop Monitoring',\n",
    "    disabled=True,\n",
    "    button_style='danger',\n",
    "    tooltip='Stop performance monitoring',\n",
    "    icon='stop'\n",
    ")\n",
    "\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "# Monitoring state\n",
    "monitoring_active = False\n",
    "monitoring_thread = None\n",
    "\n",
    "def start_monitoring(button):\n",
    "    global monitoring_active, monitoring_thread\n",
    "    \n",
    "    if monitoring_active:\n",
    "        return\n",
    "    \n",
    "    # Update monitor settings\n",
    "    monitor.interval = monitoring_interval.value\n",
    "    \n",
    "    # Disable start button, enable stop button\n",
    "    start_button.disabled = True\n",
    "    stop_button.disabled = False\n",
    "    \n",
    "    with output_widget:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"ğŸš€ Starting monitoring for {monitoring_duration.value} seconds...\")\n",
    "        print(f\"ğŸ“Š Monitoring interval: {monitoring_interval.value} seconds\")\n",
    "        print(\"ğŸ”„ Monitoring in progress...\")\n",
    "    \n",
    "    monitoring_active = True\n",
    "    \n",
    "    # Start monitoring in a separate thread\n",
    "    def monitor_task():\n",
    "        monitor.start_monitoring(duration=monitoring_duration.value)\n",
    "        \n",
    "        # Update UI when done\n",
    "        start_button.disabled = False\n",
    "        stop_button.disabled = True\n",
    "        \n",
    "        with output_widget:\n",
    "            print(\"âœ… Monitoring completed!\")\n",
    "            print(f\"ğŸ“ˆ Collected {len(monitor.data)} data points\")\n",
    "    \n",
    "    monitoring_thread = threading.Thread(target=monitor_task)\n",
    "    monitoring_thread.daemon = True\n",
    "    monitoring_thread.start()\n",
    "\n",
    "def stop_monitoring(button):\n",
    "    global monitoring_active\n",
    "    \n",
    "    if not monitoring_active:\n",
    "        return\n",
    "    \n",
    "    monitor.running = False\n",
    "    monitoring_active = False\n",
    "    \n",
    "    # Update buttons\n",
    "    start_button.disabled = False\n",
    "    stop_button.disabled = True\n",
    "    \n",
    "    with output_widget:\n",
    "        print(\"ğŸ›‘ Monitoring stopped by user\")\n",
    "        print(f\"ğŸ“ˆ Collected {len(monitor.data)} data points\")\n",
    "\n",
    "start_button.on_click(start_monitoring)\n",
    "stop_button.on_click(stop_monitoring)\n",
    "\n",
    "# Display controls\n",
    "print(\"ğŸ›ï¸  Monitoring Controls:\")\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([monitoring_duration, monitoring_interval]),\n",
    "    widgets.HBox([start_button, stop_button]),\n",
    "    output_widget\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36dc67",
   "metadata": {},
   "source": [
    "## 7. Analyze Performance Data\n",
    "\n",
    "After monitoring, we'll analyze the collected performance data to understand:\n",
    "\n",
    "- **CPU Usage Patterns**: How each runtime utilizes CPU resources\n",
    "- **Memory Consumption**: Memory usage patterns and efficiency  \n",
    "- **Resource Stability**: Consistency of resource usage over time\n",
    "- **Health Status**: Application availability and responsiveness\n",
    "- **Comparative Performance**: Direct comparisons between runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f5318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze performance data\n",
    "def load_latest_data():\n",
    "    \"\"\"Load the most recent performance data\"\"\"\n",
    "    data_dir = \"/app/data\"\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(\"âŒ Data directory not found\")\n",
    "        return None\n",
    "    \n",
    "    # Find the most recent CSV file\n",
    "    csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    if not csv_files:\n",
    "        # Try to get data from monitor if available\n",
    "        if hasattr(monitor, 'data') and monitor.data:\n",
    "            print(\"ğŸ“Š Using data from current monitoring session\")\n",
    "            return monitor.get_dataframe()\n",
    "        else:\n",
    "            print(\"âŒ No performance data found\")\n",
    "            return None\n",
    "    \n",
    "    # Get the most recent file\n",
    "    latest_file = max(csv_files, key=lambda f: os.path.getctime(os.path.join(data_dir, f)))\n",
    "    file_path = os.path.join(data_dir, latest_file)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ Loading data from: {latest_file}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load performance data\n",
    "performance_df = load_latest_data()\n",
    "\n",
    "if performance_df is not None:\n",
    "    print(f\"âœ… Loaded {len(performance_df)} data points\")\n",
    "    print(f\"ğŸ“… Time range: {performance_df['timestamp'].min()} to {performance_df['timestamp'].max()}\")\n",
    "    print(f\"â±ï¸  Duration: {(performance_df['timestamp'].max() - performance_df['timestamp'].min()).total_seconds():.0f} seconds\")\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(\"\\nğŸ“‹ Data Overview:\")\n",
    "    display(performance_df.head())\n",
    "    \n",
    "    print(\"\\nğŸ“Š Container Distribution:\")\n",
    "    container_counts = performance_df['container_name'].value_counts()\n",
    "    display(container_counts)\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  No performance data available. Please run monitoring first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea811fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance statistics\n",
    "if performance_df is not None:\n",
    "    print(\"ğŸ“Š Performance Statistics Summary:\\n\")\n",
    "    \n",
    "    # Group by container for analysis\n",
    "    stats_by_container = performance_df.groupby('container_name').agg({\n",
    "        'container_cpu_percent': ['mean', 'std', 'min', 'max'],\n",
    "        'container_memory_usage_mb': ['mean', 'std', 'min', 'max'],\n",
    "        'container_memory_percent': ['mean', 'std', 'min', 'max'],\n",
    "        'container_healthy': ['mean']  # Percentage of time healthy\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    stats_by_container.columns = ['_'.join(col).strip() for col in stats_by_container.columns]\n",
    "    stats_by_container = stats_by_container.rename(columns={\n",
    "        'container_healthy_mean': 'health_percentage'\n",
    "    })\n",
    "    \n",
    "    display(stats_by_container)\n",
    "    \n",
    "    # Create summary comparison\n",
    "    print(\"\\nğŸ† Performance Comparison (Lower is Better for CPU/Memory):\")\n",
    "    \n",
    "    summary_stats = []\n",
    "    for container in performance_df['container_name'].unique():\n",
    "        container_data = performance_df[performance_df['container_name'] == container]\n",
    "        \n",
    "        runtime_name = container.replace('-nextjs-app', '').replace('-', ' ').title()\n",
    "        \n",
    "        summary_stats.append({\n",
    "            'Runtime': runtime_name,\n",
    "            'Avg CPU (%)': container_data['container_cpu_percent'].mean(),\n",
    "            'Avg Memory (MB)': container_data['container_memory_usage_mb'].mean(),\n",
    "            'Avg Memory (%)': container_data['container_memory_percent'].mean(),\n",
    "            'CPU Stability (Ïƒ)': container_data['container_cpu_percent'].std(),\n",
    "            'Memory Stability (Ïƒ)': container_data['container_memory_usage_mb'].std(),\n",
    "            'Health %': (container_data['container_healthy'].mean() * 100),\n",
    "            'Data Points': len(container_data)\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_stats).round(2)\n",
    "    \n",
    "    # Sort by average CPU usage\n",
    "    summary_df = summary_df.sort_values('Avg CPU (%)')\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Determine winners\n",
    "    print(\"\\nğŸ¥‡ Performance Winners:\")\n",
    "    print(f\"   Lowest CPU Usage: {summary_df.iloc[0]['Runtime']} ({summary_df.iloc[0]['Avg CPU (%)']}%)\")\n",
    "    print(f\"   Lowest Memory Usage: {summary_df.loc[summary_df['Avg Memory (MB)'].idxmin(), 'Runtime']} ({summary_df['Avg Memory (MB)'].min():.1f} MB)\")\n",
    "    print(f\"   Most Stable CPU: {summary_df.loc[summary_df['CPU Stability (Ïƒ)'].idxmin(), 'Runtime']} (Ïƒ = {summary_df['CPU Stability (Ïƒ)'].min():.2f})\")\n",
    "    print(f\"   Best Health Score: {summary_df.loc[summary_df['Health %'].idxmax(), 'Runtime']} ({summary_df['Health %'].max():.1f}%)\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Cannot calculate statistics - no performance data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14a3941",
   "metadata": {},
   "source": [
    "## 8. Visualize Results\n",
    "\n",
    "Create comprehensive visualizations to compare the performance characteristics of each runtime:\n",
    "\n",
    "- **Time Series Plots**: CPU and memory usage over time\n",
    "- **Box Plots**: Distribution of resource usage\n",
    "- **Bar Charts**: Average performance metrics comparison\n",
    "- **Correlation Analysis**: Relationship between CPU and memory usage\n",
    "- **Performance Dashboard**: Interactive multi-metric view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f77945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "if performance_df is not None:\n",
    "    \n",
    "    # Set up color palette for runtimes\n",
    "    runtime_colors = {\n",
    "        'node-nextjs-app': '#68A063',    # Node.js green\n",
    "        'deno-nextjs-app': '#000000',    # Deno black  \n",
    "        'bun-nextjs-app': '#FBF0DF'      # Bun cream\n",
    "    }\n",
    "    \n",
    "    # 1. Time Series Plot - CPU Usage\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('CPU Usage Over Time', 'Memory Usage Over Time', \n",
    "                       'CPU Usage Distribution', 'Memory Usage Distribution'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # CPU time series\n",
    "    for container in performance_df['container_name'].unique():\n",
    "        container_data = performance_df[performance_df['container_name'] == container]\n",
    "        runtime_name = container.replace('-nextjs-app', '').replace('-', ' ').title()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=container_data['timestamp'],\n",
    "                y=container_data['container_cpu_percent'],\n",
    "                mode='lines+markers',\n",
    "                name=f'{runtime_name} CPU',\n",
    "                line=dict(color=runtime_colors.get(container, '#1f77b4')),\n",
    "                showlegend=True\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Memory time series\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=container_data['timestamp'],\n",
    "                y=container_data['container_memory_usage_mb'],\n",
    "                mode='lines+markers',\n",
    "                name=f'{runtime_name} Memory',\n",
    "                line=dict(color=runtime_colors.get(container, '#1f77b4'), dash='dot'),\n",
    "                showlegend=True\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # CPU distribution\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=container_data['container_cpu_percent'],\n",
    "                name=f'{runtime_name}',\n",
    "                marker_color=runtime_colors.get(container, '#1f77b4'),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Memory distribution  \n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=container_data['container_memory_usage_mb'],\n",
    "                name=f'{runtime_name}',\n",
    "                marker_color=runtime_colors.get(container, '#1f77b4'),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"Next.js Runtime Performance Comparison\",\n",
    "        title_x=0.5,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Time\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Time\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Runtime\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Runtime\", row=2, col=2)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"CPU Usage (%)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Memory Usage (MB)\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"CPU Usage (%)\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Memory Usage (MB)\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Cannot create visualizations - no performance data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ebcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison charts\n",
    "if performance_df is not None:\n",
    "    \n",
    "    # Prepare data for comparison charts\n",
    "    comparison_data = []\n",
    "    for container in performance_df['container_name'].unique():\n",
    "        container_data = performance_df[performance_df['container_name'] == container]\n",
    "        runtime_name = container.replace('-nextjs-app', '').replace('-', ' ').title()\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Runtime': runtime_name,\n",
    "            'Avg_CPU': container_data['container_cpu_percent'].mean(),\n",
    "            'Avg_Memory_MB': container_data['container_memory_usage_mb'].mean(),\n",
    "            'Avg_Memory_Pct': container_data['container_memory_percent'].mean(),\n",
    "            'CPU_Std': container_data['container_cpu_percent'].std(),\n",
    "            'Memory_Std': container_data['container_memory_usage_mb'].std()\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Create matplotlib subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Performance Comparison: Node.js vs Deno vs Bun', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Color scheme\n",
    "    colors = ['#68A063', '#000000', '#FBF0DF']  # Node, Deno, Bun\n",
    "    \n",
    "    # 1. Average CPU Usage\n",
    "    bars1 = axes[0, 0].bar(comparison_df['Runtime'], comparison_df['Avg_CPU'], color=colors)\n",
    "    axes[0, 0].set_title('Average CPU Usage (%)')\n",
    "    axes[0, 0].set_ylabel('CPU Usage (%)')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars1, comparison_df['Avg_CPU']):\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                       f'{value:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Average Memory Usage\n",
    "    bars2 = axes[0, 1].bar(comparison_df['Runtime'], comparison_df['Avg_Memory_MB'], color=colors)\n",
    "    axes[0, 1].set_title('Average Memory Usage (MB)')\n",
    "    axes[0, 1].set_ylabel('Memory Usage (MB)')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars2, comparison_df['Avg_Memory_MB']):\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                       f'{value:.0f}MB', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. CPU Stability (Lower standard deviation = more stable)\n",
    "    bars3 = axes[1, 0].bar(comparison_df['Runtime'], comparison_df['CPU_Std'], color=colors)\n",
    "    axes[1, 0].set_title('CPU Usage Stability (Lower = Better)')\n",
    "    axes[1, 0].set_ylabel('Standard Deviation')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars3, comparison_df['CPU_Std']):\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{value:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Memory Stability\n",
    "    bars4 = axes[1, 1].bar(comparison_df['Runtime'], comparison_df['Memory_Std'], color=colors)\n",
    "    axes[1, 1].set_title('Memory Usage Stability (Lower = Better)')\n",
    "    axes[1, 1].set_ylabel('Standard Deviation (MB)')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars4, comparison_df['Memory_Std']):\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                       f'{value:.1f}MB', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance ranking\n",
    "    print(\"ğŸ† Performance Rankings:\\n\")\n",
    "    \n",
    "    # Rank by CPU usage (lower is better)\n",
    "    cpu_ranking = comparison_df.sort_values('Avg_CPU')[['Runtime', 'Avg_CPU']]\n",
    "    print(\"CPU Usage (Lower is Better):\")\n",
    "    for i, (_, row) in enumerate(cpu_ranking.iterrows(), 1):\n",
    "        medal = \"ğŸ¥‡\" if i == 1 else \"ğŸ¥ˆ\" if i == 2 else \"ğŸ¥‰\"\n",
    "        print(f\"  {medal} {i}. {row['Runtime']}: {row['Avg_CPU']:.2f}%\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Rank by memory usage (lower is better)\n",
    "    memory_ranking = comparison_df.sort_values('Avg_Memory_MB')[['Runtime', 'Avg_Memory_MB']]\n",
    "    print(\"Memory Usage (Lower is Better):\")\n",
    "    for i, (_, row) in enumerate(memory_ranking.iterrows(), 1):\n",
    "        medal = \"ğŸ¥‡\" if i == 1 else \"ğŸ¥ˆ\" if i == 2 else \"ğŸ¥‰\"\n",
    "        print(f\"  {medal} {i}. {row['Runtime']}: {row['Avg_Memory_MB']:.1f} MB\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Cannot create comparison charts - no performance data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation and efficiency analysis\n",
    "if performance_df is not None:\n",
    "    \n",
    "    print(\"ğŸ”¬ Advanced Performance Analysis:\\n\")\n",
    "    \n",
    "    # CPU vs Memory correlation analysis\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Scatter plot: CPU vs Memory for each runtime\n",
    "    for container in performance_df['container_name'].unique():\n",
    "        container_data = performance_df[performance_df['container_name'] == container]\n",
    "        runtime_name = container.replace('-nextjs-app', '').replace('-', ' ').title()\n",
    "        color = runtime_colors.get(container, '#1f77b4')\n",
    "        \n",
    "        axes[0].scatter(container_data['container_cpu_percent'], \n",
    "                       container_data['container_memory_usage_mb'],\n",
    "                       label=runtime_name, alpha=0.7, color=color, s=50)\n",
    "        \n",
    "        # Calculate correlation\n",
    "        correlation = container_data['container_cpu_percent'].corr(container_data['container_memory_usage_mb'])\n",
    "        print(f\"{runtime_name} CPU-Memory Correlation: {correlation:.3f}\")\n",
    "    \n",
    "    axes[0].set_xlabel('CPU Usage (%)')\n",
    "    axes[0].set_ylabel('Memory Usage (MB)')\n",
    "    axes[0].set_title('CPU vs Memory Usage Correlation')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Resource efficiency radar chart (using matplotlib)\n",
    "    # Calculate efficiency metrics (lower values = better efficiency)\n",
    "    efficiency_metrics = {}\n",
    "    for container in performance_df['container_name'].unique():\n",
    "        container_data = performance_df[performance_df['container_name'] == container]\n",
    "        runtime_name = container.replace('-nextjs-app', '').replace('-', ' ').title()\n",
    "        \n",
    "        # Normalize metrics (0-1 scale, lower is better)\n",
    "        avg_cpu = container_data['container_cpu_percent'].mean()\n",
    "        avg_memory = container_data['container_memory_usage_mb'].mean()\n",
    "        cpu_stability = container_data['container_cpu_percent'].std()\n",
    "        memory_stability = container_data['container_memory_usage_mb'].std()\n",
    "        \n",
    "        efficiency_metrics[runtime_name] = {\n",
    "            'CPU Usage': avg_cpu,\n",
    "            'Memory Usage': avg_memory / 100,  # Scale to similar range\n",
    "            'CPU Stability': cpu_stability,\n",
    "            'Memory Stability': memory_stability / 10  # Scale to similar range\n",
    "        }\n",
    "    \n",
    "    # Create efficiency comparison\n",
    "    metric_names = list(efficiency_metrics[list(efficiency_metrics.keys())[0]].keys())\n",
    "    x_pos = np.arange(len(metric_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, (runtime, metrics) in enumerate(efficiency_metrics.items()):\n",
    "        values = list(metrics.values())\n",
    "        color = ['#68A063', '#000000', '#FBF0DF'][i]\n",
    "        axes[1].bar(x_pos + i * width, values, width, label=runtime, color=color, alpha=0.8)\n",
    "    \n",
    "    axes[1].set_xlabel('Metrics')\n",
    "    axes[1].set_ylabel('Score (Lower = Better)')\n",
    "    axes[1].set_title('Resource Efficiency Comparison')\n",
    "    axes[1].set_xticks(x_pos + width)\n",
    "    axes[1].set_xticklabels(metric_names, rotation=45)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate overall efficiency score\n",
    "    print(\"\\nğŸ“Š Overall Efficiency Scores (Lower = Better):\")\n",
    "    overall_scores = {}\n",
    "    for runtime, metrics in efficiency_metrics.items():\n",
    "        # Weighted average (equal weights for simplicity)\n",
    "        score = sum(metrics.values()) / len(metrics)\n",
    "        overall_scores[runtime] = score\n",
    "        \n",
    "    # Sort by efficiency (lower is better)\n",
    "    sorted_scores = sorted(overall_scores.items(), key=lambda x: x[1])\n",
    "    \n",
    "    for i, (runtime, score) in enumerate(sorted_scores, 1):\n",
    "        medal = \"ğŸ¥‡\" if i == 1 else \"ğŸ¥ˆ\" if i == 2 else \"ğŸ¥‰\"\n",
    "        print(f\"  {medal} {i}. {runtime}: {score:.2f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ† Most Efficient Runtime: {sorted_scores[0][0]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Cannot perform advanced analysis - no performance data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28ccace",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "This notebook provides a comprehensive comparison of Next.js performance across three JavaScript runtimes. Based on the analysis, you can make informed decisions about which runtime best fits your specific needs.\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **CPU Performance**: Shows which runtime uses the least CPU resources under load\n",
    "2. **Memory Efficiency**: Identifies the most memory-efficient runtime \n",
    "3. **Stability**: Measures consistency of resource usage over time\n",
    "4. **Health/Reliability**: Tracks application uptime and responsiveness\n",
    "\n",
    "### How to Use This Analysis:\n",
    "\n",
    "- **For Production Deployment**: Choose the runtime with the best overall efficiency and stability\n",
    "- **For Development**: Consider the balance between performance and developer experience\n",
    "- **For Scaling**: Focus on the runtime with the most predictable resource usage patterns\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Load Testing**: Run this analysis under different load conditions\n",
    "2. **Extended Monitoring**: Monitor for longer periods to identify trends\n",
    "3. **Real-World Scenarios**: Test with actual application workloads\n",
    "4. **Cost Analysis**: Consider infrastructure costs based on resource usage\n",
    "\n",
    "The performance characteristics may vary based on your specific application, traffic patterns, and infrastructure setup."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
